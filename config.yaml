# CogniBench Configuration File
# Add your evaluation settings here.
# Example structure (adjust based on actual requirements):

# --- LLM Client Configuration ---
# Specifies the Large Language Model provider and settings to use for generating responses
# (if applicable) and potentially for judging.
llm_client:
  provider: openai # e.g., openai, anthropic, local_llm
  model: gpt-4o    # Specific model name (e.g., gpt-4o, claude-3-opus-20240229)
  # API Key for the provider. Using environment variable substitution is recommended for security.
  # The actual key should be stored in a .env file (e.g., OPENAI_API_KEY="sk-...")
  # and loaded by the application (e.g., using python-dotenv).
  api_key: ${OPENAI_API_KEY}
# --- Input Data Configuration ---
input_options:
  # Path to the PROCESSED/INGESTED data file required by the core evaluation script.
  # This file should contain data in the format expected by CogniBench's workflow
  # (e.g., prompt, ideal_response, etc.), typically generated by an ingestion script.
  # NOTE: When using wrapper scripts like 'run_batch_evaluation.py', this value is often
  # overridden by a command-line argument (e.g., --input-data) that provides the path
  # to the dynamically generated ingested file.
  data_file: "data/default_ingested_data.json" # Example default path

# --- Structuring Settings ---
structuring_settings:
  # Specifies the model to be used for structuring responses.
  structuring_model: gpt-4o
  # Path to the text file containing the prompt template used for structuring.
  prompt_template: "prompts/structuring/Math-L1-Structuring-v1.0.txt"

# --- Evaluation Settings ---
# Configures the LLM-as-a-judge process.
evaluation_settings:
  # Specifies the model to be used as the 'judge' for evaluating responses.
  # This can be the same as or different from the model used for generating responses.
  judge_model: gpt-4o
  # Path to the text file containing the prompt template used to instruct the judge model.
  prompt_template: "prompts/judging/Math-L1-Judging-v1.0.txt" # Updated to correct template path
  # List of criteria names expected in the judge LLM's JSON output (under 'evaluation').
  # The parser will validate against these (case-insensitive matching).
  expected_criteria:
    - "Problem Understanding"
    - "Assumptions"
    - "Logical Implications"
    - "Results Formulae"
    - "Rigor and Completeness"
  # List of allowed score values for each criterion (case-insensitive matching).
  allowed_scores:
    - "Yes"
    - "No"
    - "Partial"

# --- Human Review Configuration ---
human_review_settings:
  flag_rules:
    flag_on_partial_score: true
    flag_on_trivial_justification: true
    flag_on_verification_failure: true

# --- Aggregation & Consistency Check Configuration ---
aggregation_settings:
  aggregation_rules:
    fail_if_any_no: true
    partial_if_any_partial: true
    pass_if_all_yes: true
  consistency_checks:
    enable_trivial_justification_check: true
    trivial_justification_length_threshold: 10

# --- Output Configuration ---
# Defines where the evaluation results should be saved.
output_options:
  # Path to the file where the final evaluation results will be written (usually JSON).
  results_file: "data/evaluation_results.json" # Example path
